# Text Classification

**Course**: CS-433 Machine Learning

**Authors**:

- Ali Elkilesly (SCIPER 345334)
- Selim Sherif (SCIPER 346035)
- Roy Turk (SCIPER 345573)

---

## Table of Contents

1. [Project Overview](#project-overview)
2. [Repository Contents](#repository-contents)
   - [Base Model](#base-model)
   - [Advanced Model](#advanced-model)
   - [Hyperparameter Tuning](#hyperparameter-tuning)
3. [Generated Results](#generated-results)
4. [Acknowledgments](#acknowledgments)

---

## Project Overview

This project focuses on classifying the sentiment of tweets using machine learning and transformer-based models. It is divided into two parts:

1. **Baseline Models**: Traditional machine learning techniques, including preprocessing, are used to predict tweet sentiment as positive or negative.
2. **Advanced Models**: Transformer-based architectures such as **BERT** and **BERTweet** are employed to achieve higher accuracy and better handle the nuances of informal text.

## Repository Contents

A comprehensive report detailing the methodology, results, and conclusions of this project will be included as a PDF in the root directory of the repository.

The repository contains a `twitter.datasets.zip` file, which should be unzipped for use.

### Base Model

The base model implementation includes the following files:

- **`main.ipynb`**: Implements the pipeline for Logistic Regression and SVM, covering data cleaning, feature extraction with TF-IDF, model training, and evaluation metrics.
- **`HP_Tuning.ipynb`**: Performs hyperparameter tuning and tests input data. Key hyperparameters for Logistic Regression and SVM are optimized using grid or random search.
- **`helpers.py`**: Provides utilities for data preprocessing (e.g., cleaning, tokenization, lemmatization), vocabulary building, and GloVe embedding integration.
- **`en-80k.txt`**: A dictionary file used for spell correction during preprocessing, containing terms and their frequencies.
- **`submission.csv`**: Stores sentiment predictions generated by the baseline models, formatted for submission.

#### Dependencies

Before running the base model, ensure the following libraries are installed:

```
!pip install numpy nltk symspellpy scikit-learn contractions
```

#### Instructions for Using the Base Model

1. **Run Notebooks**:

   - Open `main.ipynb` in Jupyter Notebook.
   - Specify the paths for training and testing datasets within the notebook:
     ```
     pos_file = "twitter-datasets/train_pos_full.txt"
     neg_file = "twitter-datasets/train_neg_full.txt"
     test_file = "twitter-datasets/test_data.txt"
     ```
   - Execute the cells sequentially to preprocess the data, train the machine learning model, and evaluate its performance. Each step of the pipeline is clearly documented within the notebook.

2. **Save Predictions**: The generated predictions will be saved in `submission.csv`.

### Advanced Model

The advanced model repository includes the following files:

- **`BERT_model.ipynb`**: Implements the fine-tuning of the BERT model for sentiment analysis.
- **`BERTweet_model.ipynb`**: Implements the fine-tuning of the BERTweet model for analyzing social media text.
- **`Model_tuning.ipynb`**: Facilitates hyperparameter tuning for both BERT and BERTweet models. Uncomment the appropriate line in the notebook to select the desired model:
  ```
  model_ckpt = "bert-base-uncased" 
  # model_ckpt = "vinai/bertweet-base"
  ```

#### Dependencies

The code was developed in the \*\*Google Colab\*\* virtual environment, which provides a cloud-based platform for executing

Python scripts. Before running the notebook, it is essential to install the required packages to ensure all dependencies

are available.

The first code cell of each notebook is executed to install the necessary libraries.

```
!pip install -U transformers
!pip install -U accelerate
!pip install -U datasets
!pip install -U bertviz
!pip install -U umap-learn
!pip install -U seaborn
!pip install -U emoji
```

#### Instructions for File Paths and Saving the Model

1. **Specify File Paths for Training and Testing Datasets**:

   Replace the placeholders with the correct file paths to the training and testing datasets. For example:

   ```
   train_neg_path = "drive/train_neg_full.txt"
   train_pos_path = "drive/train_pos_full.txt"
   test_path = "drive/test_data.txt"
   ```

2. **Model Saving**:

   After training the model, save it to the desired directory:

   ```
   trainer.save_model("bert-model")
   ```

3. **Saving Predictions to a CSV File**:

   After generating predictions using the fine-tuned model, save the results to a `.csv` file in the desired directory:

   ```
   output_path = "BERT_predictions.csv" # Define the path for the output file
   ```

## Hyperparameter Tuning

- **Baseline Models**: Use `HP_Tuning.ipynb` to explore and fine-tune hyperparameters for Logistic Regression and SVM. The results of different tests are commented within the notebook.

- **Advanced Models**: Use `Model_tuning.ipynb` to adjust hyperparameters for either BERT or BERTweet. Ensure that the desired model is selected by uncommenting the corresponding line:

  ```
  model_ckpt = "bert-base-uncased"
  # model_ckpt = "vinai/bertweet-base"
  ```

## Generated Results

### Baseline Model:

- `submission.csv`: Contains predictions generated by the trained model on the test set.

### Advanced Model:

- `BERT_predictions.csv`: Contains predictions generated using the **BERT** model.
- `BERTweet_predictions.csv`: Contains predictions generated using the **BERTweet** model.
- `best_predictions.csv`: Represents the predictions from the best-performing model (BERTweet) that were submitted to the test site on **AI Crowd**. Note that exact results may vary due to variations in dataset splitting.

## Acknowledgments

This project leverages:

- SymSpell for spelling correction.
- GloVe embeddings for semantic vectorization.
- NLTK for text processing.
- Transformers Library for advanced model fine-tuning.

For further inquiries or contributions, please contact the authors.


